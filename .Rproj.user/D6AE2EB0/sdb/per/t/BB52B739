{
    "contents" : "---\ntitle: \"Adaptability Study - Overview of Data Analysis\"\nauthor: \"Kevin A. Ryan\"\ndate: \"Monday, July 20, 2015\"\noutput:\n  word_document: default\n  html_document:\n    fig_caption: yes\n    highlight: kate\n    theme: journal\n---\n\n### Section #1 - Overview of the Adaptability Study\nwarning=FALSE, error=FALSE, message=FALSE,\n\n\n\n---\n\n***The Data Science Pipeline ***\n\n\n\n\nScientific Question ->\n  Protocol ->\n    Experiment ->\n      Measured Data ->              <- PROCESSING CODE\n        Analytic Data ->            <- ANALYTIC CODE\n          Computational Results ->  <- PRESENTATION CODE\n            Figures/Tables/Summaries-> \n              Article \n\n\n***why reproducible research on Adaptability is critical***\n\n1. Don't Do Things By Hand\nIf this chapter could be boiled down to one rule, it might be \"Don't do things by hand\". What do I mean by that? Here are a few examples that are common, but are bad practice:\n. Editing spreadsheets of data to \"clean it up\". Often this is doen to remove outliers, do quality assurance or quality control checks (QA/QC), or validating individual data entries\n. Editing tables or figures (e.g. rounding, formatting) to make then look better\n. Downloading data from a web site using a web browser\n. Moving data around your computer\n. Splitting or reformatting data files\n\nOften, the motiviation for doing all of the above things is that \"We're just going to do this once.\" The thinking is that if the activity is only going to be done once, it doesn't need to be automated (i.e. programmed into a computer).\n\nBut programming a procedure into a computer is not necessarily about automation. It is also about documentation. The problem with things that are done by hand, is that things done by hand need to be precisely documented (this is harder than it sounds). Often, it can very difficult to communicate to someone what was done after the fact. It can be easy to miss a step that \"isn't important\" when in fact it is.\n\n2. Don't Point And Click\nPointing and clicking is obviously related to doing things by hand. Most modern operating systems have a windowing interface that allow you to click on menus that can lead to automated built-in routines. Many data processing and statistical analysis packages have graphical user interfaces (GUIs) that simplify the use of the program, but the actions you take with a GUI can be difficult for others to reproduce because there's not necessarily a log of what was clicked.\n\nSome GUIs for statistical analysis packages produce a log file or script which includes equivalent commands for reproducing the behavior of the GUI, but this is by no means the standard. In general, be careful with data analysis software that is highly interactive. There is often a trade-off between the ease of use of a software package and the tendency to lead to non-reproducible results.\n\nOf course, this doesn't mean that all interactive software. Some software has to be interactive, like text editors or word processors, and that's fine. It's just when the software must be used to conduct data analysis, you must be careful not to be seduced by the ease-of-use of an interactive interface.\n\n3. Teach a Computer\nThe opposite of doing things by hand is teaching a computer to do something. Computers need very precise instructions to accomplish a task so there's no room for ambiguity. This is a Good Thing if your goal is to make your procedures and processes reproducible. If something needs to be done as part of your analysis or investigation, try to teach your computer to do it, even if you only need to do it once. In the end, teaching a computer to do something almost guarantees reproducibilty\n\n*** New Structure of the Document ***\n\n. Defining the question\n. Defining the ideal dataset\n. Determining what data you can access\n. Obtaining the data\n. Cleaning the data\n. Exploratory data analysis\n. Statistical prediction/modeling\n. Interpretation of results\n. Challenging of results\n. Synthesis and write up\n. Creating reproducible code\n\n***Summary of Study Question***\n\nThe purpose of the study is to determine, for the U.S. Army Asymmetric Warfare Group (AWG) quantitative metrics of military-relevant adaptive behavior. We expect a positive relationship between performance on the executive function tasks and military-relevant adaptive test tasks. We also expect a negative relationship between adaptive performance and stress reactivity. Specifically, higher adaptive performance will be negatively correlated with metrics of cardiovascular, neuroendocrine, and electro-dermal stress response indices.\n\n***A Philosophy of Data Analysis***\n\nIn many ways, according to Peng (2015), the relationship between the Data Analyst and the Principle Investigator is similar to the relationship Filmmakers have with Editors when making a movie.  He notes: \"In the editing room, the Filmmaker and the Editor can experiment with different versions of different scenes to see which dialogue sounds better, which jokes are funnier,or which scenes are more dramatic. Weak scenes might get dropped, and scenes that are particularly powerful might get extended or re-shot. This \"rough cut\" of the film is put together quickly so that important decisions can be made about what to pursue further and where to back off. Finer details like color correction or motion graphics might not be implemented at this point. Ultimately, this rough cut will help the director and editor create the \"final cut\", which is what the audience will ultimately view.\"\n\nThe JHUAPL Data Analyst and JHUAPL Principle Investigator have a similar relationship on the adaptability study. Exploratory data analysis is what occurs in the \"editing room\" of a research project or any data-based investigation. The JHUAPL team will make a \"rough cut\" for a data analysis, the purpose of which is very similar to that in the film editing room.  The Data Analyst will seek to identify relationships between variables that are particularly interesting or unexpected, checking to see if there is any evidence for or against a stated hypothesis, checking for problems with the collected data.  The Data Analyst will always use a subset of data collected by the Principle Investigator, and will also advise the investigator to make critical decisions about what is interesting to follow up on and what probably is not worth pursuing because the data don't provide the evidence. \n\n\nThe JHUAPL Team will employ neuropsychological assessment of executive function to isolate the relationship between specific metrics (inhibition, problem solving and working memory) and adaptability. Participants will also complete a set of computer-based, mission-relevant adaptive test tasks developed to echo elements of the AWG adaptive training program as well as authentic military challenges in order to gauge overall participant adaptable performance. The following table summarizes, for each type of data collected in the Adaptability experiement, the source of the data and why those data are important given the study's purpose.\n\n***Summary of Study Data***\n\nData Type         | Data Source                                     | Outcome/Importance\n----------------- | ------------------------------------------------|---------------------------\nDemographic Data  | Basic self-reported demographic characteristics | Characterize the sample\nBehavioral Assessments  | Positive/Negative Affect Schedule (PANAS) Short Stress State Questionnaire (SSSQ) State Trait Anxiety Inventory (STAI) Profile of Mood States (POMS) Dispositional Resilience Scale (DRS) | Measure stress, anxiety, mood, and resilience disposition \nExecutive Function Tests| Iowa Gambling Test; N-Back; Eriksen Flanker  | Tests decision making ability, cognitive flexibility, working memory, and response inhibition\nPerformance | Written responses to military test tasks  | Adaptability to consistent and inconsistent stimuli in a military context\nPsychophysiological | Electrocardiography (ECG); Galvanic Skin Response (GSR); Cortisol/??-Amylase | Measure increased and decreased stress, as both an immediate response, and over the study period\nSelf-reports  | Written responses after completion of military test tasks | Individual's perceived effectiveness, focus and confidence on the adaptability tasks\n\n***Data Collection Process***\n\nThe process for collecting the data in the above table will follow a strict protocol, which is highlighted in this section.  The participants in the adaptability study will spend approximately 3 hours in the JHUAPL lab. \n\n- Study participant arrives at JHU/APL testing room.\n- The study team will provide an overview of the experiment and study procedures including the psychophysiological measures, sensors and procedures. These instructions will be provided by PowerPoint and study staff will follow an experimenter script across the testing session to assure ensure consistency with each study participant and across data collections.\n- The participant will provide written informed consent. Time has been planned to assure that the participant's questions are addressed at this time and throughout the study session.\n- A saliva sample will be taken immediately following informed consent and then periodically 7 more times (for a total of 8) throughout the testing.\n- A research team member will then instrument the participant with the ECG (2 sensors on belly and collar area) and GSR (two sensors on two fingers of non-dominant hand) sensors. We will use the BioPac sensory system to acquire these data. Baseline measures of the physiological signals will be acquired in order to test for signal integrity and acquire resting baseline values on which data checks may be made.\n* The participant will complete initial behavioral assessments including:\n    + a demographics questionnaire, \n    + the Short Stress State Questionnaire (SSSQ), \n    + the State Trait Anxiety Inventory (STAI), \n    + the Profile of Mood States (POMS), \n    + the Positive and Negative Affect Schedule (PANAS), and \n    + the Dispositional Resilience Scale (DRS). \n  \n* The participant will complete the neuropsychological tasks:\n    + N-back, \n    + Ericksen Flanker, \n    + IOWA Gambling Task \n    \n- The participant will compete the Adaptive Test Tasks. \n- When the testing session is completed the ECG and GSR sensors will be removed.\n\n###Section #2 - Data Diagnostics\n---\n\nFor this study, the JHUAPL Data Analyst will work closely with the JHUAPL Principle Investigator to ensure the overall objectives of the study are met and the resulting findings are defensible, repeatable, insightful and of the highest possible quality. \n\nThe primary objective of the analysis is to determine objective metrics of adaptive behavior at the individual level via neuropsychological assessments, controlled, mission-relevant test tasks, and psychophysiological (electro-cardiovascular, skin conductance and neuroendocrine) responses observed during challenges.  To meet this objective, the JHUAPL study team has developed a formal process leveraging the work by Peng (2015). \n\n***Steps in Data Diagnostics***\n\n1. **Revisit the study question**.  JHUAPL believes that formulating a strong question is important to guide the exploratory data analysis process and to limit the exponential number of paths that can be taken with a dataset. In particular, a sharp question or hypothesis can serve as a dimension reduction tool that can eliminate variables that are not immediately relevant to the question. \n\n|Study Question|  \n|-------|\n|The primary objective of the analysis is to determine objective metrics of adaptive behavior at the individual level via neuropsychological assessments, controlled, mission-relevant test tasks, and psychophysiological responses observed during challenges. The JHUAPL study team expects a positive relationship between performance on the executive function tasks and military-relevant adaptive test tasks. We also expect a negative relationship between adaptive performance and stress reactivity.| \n\n2. **Read in data**. The next task in the JHUAPL exploratory data analysis is to read in data from the experiment. Peng (2015) warns that sometimes, the data will come in a  messy format and the data analyst need to do some cleaning. As such, the JHUAPL data analysis team has taken the following steps to ensure tidy and clean data. \n\nThe Code below reads in the lookup table from above and the design matrix. This does not need to be run for each participant. \n```{r}\n#EF_lookup <- read.csv(\"C:/Users/ryanka1/Desktop/adaptability Data/EF_lookup.csv\")\n#EF_Design_Matrix<-read.csv(\"C:/Users/ryanka1/Desktop/adaptability Data/EF_Design_Matrix.csv\")\n```\n\nThe following steps are required to load in a NEW data set.  The code that follows will automatically pull in data on: Demographics, Behavioral Assessments, Executive Function Tests, Performance on Adaptive Tasks, Psychophysiological traits, and Self-reports.  First, enter in the Participant ID. \n```{r}\nParticipantID<-1\n```\n\nFor each participant ID, the code will pull in all Demographic data collected in the experiment. \n```{r}\n#Code for ----->Demographic \n```\n\nFor each participant ID, the code below will pull in all data collected from Behavioral Assessments. \n```{r, echo= FALSE, eval=FALSE}\nFilePath<-file.path(\"//aplfsfrontier.dom1.jhuapl.edu/Projects/AWG/Adaptability/Data/Experiment Results\", ParticipantID, \"ba.csv\" )\nba <- read.csv(FilePath)\n#Code for ----->the Short Stress State Questionnaire (SSSQ)\nSSSQ<-ba[ ,c(1, 4:12)]\n#Code for ----->the State Trait Anxiety Inventory (STAI)\nSTAI<-ba[ ,c(1, 13)]\n#Code for ----->the Profile of Mood States (POMS)\nPOMS<-ba[ ,c(1, 4:12)]\n#Code for ----->the Positive and Negative Affect Schedule (PANAS)\nPANAS<-ba[ ,c(1, 2,3)]\n#Code for ----->the Dispositional Resilience Scale (DRS).\nDRS<-ba[ ,c(1, 21:24)]\n```\n\nFor each participant ID, the code will pull in all data on the Executive Function Tests\n```{r eval=FALSE}\n#Code  for ----->Erickson Flanker \nFilePath<-file.path(\"//aplfsfrontier.dom1.jhuapl.edu/Projects/AWG/Adaptability/Data/Experiment Results\", ParticipantID, \"EF.csv\" )\nEF <- read.csv(FilePath)\nEF1<-EF[ ,c(36, 1, 2, 3, 5, 13, 15, 25, 26, 32, 39)]\ncnames<-c(\"Trial\", \"Experiment Name\", \"Subject\", \"Session\", \"DataFile\", \"Session Date\", \n          \"Session Time\", \"Correct\", \"CRESP\", \"ReactionTime\", \n          \"Arrows\" )\ncolnames(EF1) <- cnames #rename the columns \n\nEF2 <-(merge(EF_Design_Matrix, EF1, by = 'Trial',  all.y = TRUE))#Lookup Previous/Next from Design Matrix, and look up Congruent/Incongruent from lookup table\nEF3 <- (merge(EF_lookup, EF2, by = 'Arrows', all.y = TRUE))\nEF3$ReactionTime<-ifelse(EF3$Current == \"C\", rnorm(500, mean = 514, sd = 154), rnorm(500, mean = 500, sd = 144) )\nEF3$Correct<-ifelse(EF3$Current == \"C\", rbinom(500, size=1, prob=.8), rbinom(500, size=1, prob=.90) )\n  \n#Code for ----->n-Back\nFilePath<-file.path(\"//aplfsfrontier.dom1.jhuapl.edu/Projects/AWG/Adaptability/Data/Experiment Results\", ParticipantID, \"nback.csv\" )\nnBack <- read.csv(FilePath)\nnBack1<-nBack[ ,c(1, 2, 3, 45, 5, 13, 15, 25, 26, 32, 34, 35)]\ncnames<-c(\"Experiment Name\", \"Subject\", \"Session\", \"Trial\", \"DataFile\", \"Session Date\", \"Session Time\", \"Correct\", \"CRESP\", \"ReactionTime\", \"ACC\", \"CRESP2\" )\ncolnames(nBack1) <- cnames \nnBack1$ReactionTime<-rnorm(86, mean = 1530, sd = 350)\nnBack1$Correct<-rbinom(86, size=1, prob=.70)\n\n#Code for ----->Iowa Gambling Task \nFilePath<-file.path(\"//aplfsfrontier.dom1.jhuapl.edu/Projects/AWG/Adaptability/Data/Experiment Results\", ParticipantID, \"nback.csv\" )\nnBack <- read.csv(FilePath)\nnBack1<-nBack[ ,c(1, 2, 3, 45, 5, 13, 15, 25, 26, 32, 34, 35)]\ncnames<-c(\"Experiment Name\", \"Subject\", \"Session\", \"Trial\", \"DataFile\", \"Session Date\", \"Session Time\", \"Correct\", \"CRESP\", \"ReactionTime\", \"ACC\", \"CRESP2\" )\ncolnames(nBack1) <- cnames \nnBack1$ReactionTime<-rnorm(86, mean = 1530, sd = 350)\nnBack1$Correct<-rbinom(86, size=1, prob=.70)\n```\n\nFor each participant ID, the code below will pull in all data from the Adaptive Test Task. \n```{r}\n#Code for ----->Written responses to the test task\n\n```\n\nFor each participant ID, the code below will pull in all Psychophysiological data collected. \n```{r}\n#Code for ----->Electrocardiography (ECG)\n\n#Code for ----->Galvanic Skin Response (GSR)\n\n#Code for ----->Cortisol/alpha-Amylase\n\n```\n\n3. **Verify Rows and Columns**. At this point in the data analysis process, the JHUAPL study team will need to verify that the file named on the network drive actually contains data.   The `dim` function in R will allow the JHUAPL team to peek into the data file to ensure the number of columns and rows of data matches the analyst's expectations. The JHUAPL team believes, and Peng (2015) concurs, that steps should be put in place to identify errors throughout the exploratory data analysis process.   \nHow many rows and how many columns in the vector 'EF2?'\n```{r eval=FALSE }\ndim(EF3) #first number in the vector is the number of rows in the vector `EF1` the second is the number of columns. \ndim(SSSQ)\ndim(STAI)\ndim(PANAS)\ndim(DRS)\ndim(nBack1)\n```\n\n4. **Run str()**. JHUAPL will next run the `str()` function on the dataset. This is usually a safe operation in the sense that even with a very large dataset, running str() shouldn't take too long.The output for str() duplicates some information that we already have, like the number of rows and columns. More importantly, it will allow JHUAPL the ability to examine the classes of each of the columns to make sure they are correctly specified (i.e. numbers are numeric and strings are character, etc.). \n\nReorder columns and summarize at a high level. what does this data vector look like?  \n```{r eval=FALSE}\nEF4<-EF3[ ,c(9,5,4,6,7,14,16 )]\nstr(EF4)\nstr(nBack1)\n```\n5. **Conduct Initial Inspection of the Data**. The JHUAPL team will look at the \"beginning\" and \"end\" of a dataset to ensure the data has been read in correctly,   the data are properly formatted, and all expected data is in the file.  For time time series data, JHUAPL will make sure the dates at the beginning and end of the dataset match expectations. \n\nReview of Erickson Flanker for current participant:\n```{r eval=FALSE}\n#Experiment Name: ef = Erickson Flanker, \nhead(EF4)\ntail (EF4)\nhead(SSSQ)\nhead(STAI)\nhead(PANAS)\nhead(DRS)\n```\n\n7. **Validate with external data source**. \nMaking sure your data matches something outside of the dataset is very important. It will allow the JHUAPL team to ensure that the measurements are roughly in line with what they should be and it serves as a check on what other things might be wrong in the adaptability dataset. To do this, the JHAPL analyst will open the excel spreadsheet containing the raw data from Eprime and spot check the numbers there against numbers in R.  \n```{r eval=FALSE}\nEF4[22,5] #compare the value in row 22, column 4 of the EF2 data vector against the Excel raw file\nEF4[447,3] #compare the value in row 447, column 3 of the EF2 data vector against the Excel raw file\n```\n\n8. **Build Graphics for Single Participant**.\nWhat's the simplest answer we could provide to this question? For the moment, don't worry about whether the answer is correct, but the point is how could you provide prima facie evidence for your hypothesis or question. You may refute that evidence later with deeper analysis, but this is the first pass. Because we want to know which counties have the highest levels, it seems we need a list of counties that are ordered from highest to loweet with respect ot their levels of ozone.\nFor each of the variables in `EF2`:  what is the minimum, 25th percentile, median, 75th percentile, and maximum of the data?\n\nThis is the code for the theme to be used for graphics:\n \n```{r echo=FALSE}\nlibrary(RColorBrewer)\nlibrary(ggplot2)\n```\n\n```{r eval=FALSE}\nsummary(EF4)\n```\nIs there an impact of Trial Type (consistent or inconsistent)\n```{r eval=FALSE}\nlibrary(dplyr)\nlibrary(ggplot2)\nEF5<-filter(EF4, ReactionTime > 0) #filter out those trials where reaction time = 0\nqplot(ReactionTime, data = EF5, facets = Current ~ ., binwidth = 25, fill = Current)\n\n```\nThe red line presents the `mean(ReactionTime, na.rm=T)`Due to time constraints, the participant (Me) did not submit to the full 500 trial test for the Erickson Flanker task. As such, a majority of the data in this current set is synthetic and populated via random sampling of a uniform distribution that had the min and max determined by the participant's initial training phase of the task.  It is expected that the distribution of Response TImes for the full trial of 500 will be much more gausian in appreance.  \n\n###Section 3- Analysis of Results\n---\n** Review of Study Hypothesis **\n\nHypothesis #1: We expect a positive relationship between performance on the executive function tasks and military-relevant adaptive test tasks. \n\nHypothesis #2: We expect a negative relationship between adaptive performance and stress reactivity.  \n\n\nExample:\n\n\n```{r}\nlibrary(lattice)\nset.seed(10)\nx <- rnorm(100)\nf <- rep(0:1, each = 50)\ny <- x + f - f * x + rnorm(100, sd = 0.5)\nf <- factor(f, labels = c(\"Group 1\", \"Group 2\"))\nxyplot(y ~ x | f, panel = function(x, y, ...) {\n panel.xyplot(x, y, ...) ## First call the default panel function for 'xyplot'\n panel.abline(h = median(y), lty = 2) ## Add a horizontal line at the median\n})\n\n```\n\n```{r}\nxyplot(y ~ x | f, panel = function(x, y, ...) {\n panel.xyplot(x, y, ...) ## First call default panel function\n panel.lmline(x, y, col = 2) ## Overlay a simple linear regression line\n})\n```\n\n#Clustering??\n#Principle Component Analysis?  \n#Singular Value Decomposition?/\n\n\n** Misc Notes on Data Visualization**\n\nAdapted from the book [`Edward Tufte. (1983) \"The Visual Display of Quantitative Information\" ][2]`\n\n+ **show comparisons**. Showing comparisons is really the basis of all good scientific investigation. Evidence for a hypothesis is always relative to another competing hypothesis. A good scientist is always asking \"Compared to What?\" when confronted with a scientific claim or statement. Data graphics should generally follow this same principle. You should always be comparing at least two things.\n+ **Show causality, mechanism, explanation, systematic structure**. If possible, it's always useful to show your causal framework for thinking about a question. Generally, it's difficult to prove that one thing causes another thing even with the most carefully collected data. But it's still often useful for your data graphics to indicate what you are thinking about in terms of cause. Such a display may suggest hypotheses or refute them, but most importantly, they will raise new questions that can be followed up with new data or analyses.\n+ **Show multivariate data**. For anything that you might study, there are usually many attributes that you can measure. The point is that data graphics should attempt to show this information as much as possible, rather than reduce things down to one or two features that we can plot on a page. \n+ **Integrate of evidence**. Data graphics should make use of many modes of data presentation simultaneously, not just the ones that are familiar to you or that the software can handle. One should never let the tools available drive the analysis; one should integrate as much evidence as possible on to a graphic as possible. \n+ **Describe and document the evidence** Data graphics should be appropriately documented with labels, scales, and sources. A general rule for me is that a data graphic should tell a complete story all by itself. You should not have to refer to extra text or descriptions when interpreting a plot, if possible. Ideally, a plot would have all of the necessary descriptions attached to it. You might think that this level of documentation should be reserved for \"final\" plots as opposed to exploratory ones, but it's good to get in the habit of documenting your evidence sooner rather than later. with appropriate labels, scales, sources, source code. \n+ **Content**.  Analytical presentations ultimately stand or fall depending on the quality, relevance,\nand integrity of their content. This includes the question being asked and the evidence presented in favor of certain hypotheses. No amount of visualization magic or bells and whistles can make poor data, or more importantly, a poorly formed question, shine with clarity. Starting with a good question, developing a sound approach, and only presenting information that is necessary for answering that question, is essential to every data graphic.\n\n####Referencces\n[1]: https://leanpub.com/exdata\n[2]: http://www.edwardtufte.com/tufte/books_vdqi\nEdward Tufte (2006). Beautiful Evidence, Graphics Press LLC. www.edwardtufte.com\n\n###APPENDIX:  Data Dictionaries for all Eprime Data Collected\n---\nOnce the steps above are completed by the study participant, the Eprime software write all data collected to an .EDAT file.  The JHUAPL examiner will be required to open this *.EDAT file and extract all data to a seperate Excel file containing.  Both the EDAT file and the excel file will be saved to the electronic folder on the network [`\\\\aplfsfrontier.dom1.jhuapl.edu\\Projects\\AWG\\Adaptability`] Procedureally, the following steps need to occur to produce a single set of raw data for a single study participant: \n\n  + The name of the Excel file will be the ID# of the participant.  \n  + The File will have 3 tabs (for now)\n      + Tab 1 will be named \"nback\" and will contain all data from Eprime on the N-back task\n      + Tab 2 will be named \"ef\" and will contain all data from Eprime from the Eriksen Flanker task\n      + Tab 3 will be named \"igt\" and will contain all Eprime data from the Iowa Gambling Task \n- The data analyst will convert each excel tab into a CSV file. \n- Each CSV file will be saved in \n- The following table outlines the columns of data written to the Eprime database for one of the Executive Function data types.  Recall that there are 6 types of data (Demographic Data, Behavioral Assessments, Executive Function Tests, Adaptive Performance, Psychophysiological and Self Report).  There may be multiple tasks withing a data type.  For the example below - Eriksen Flanker is one task under the Executive Function data type.  For this set of data, all columns are listed in the table below as a reference, but not all columns will be used in the data analysis.  Eventually, a similar table will be created for the remaining data types.   \n \n \n#####Table D1:  Data Dictionary for Demographic Data (D1)\nVariable|Used in Analysis|English Name|Description\n-|-|-|- \nTBD|Y|TBD|TBD\n\n#####Table BA1:  Data Dictionary for Positive/Negative Affect Schedule (PANAS)\nEprime Variable|Used in Analysis|English Name|Description\n-|-|-|- \nIdentifier|Y|TBD|TBD\nPos. Affect|Y|TBD|TBD\nNeg. Affect|Y|TBD|TBD\n\n\n#####Table BA2:  Data Dictionary for Short Stress State Questionnaire (SSSQ) \nEprime Variable|Used in Analysis|English Name|Description\n-|-|-|- \nTBD|Y|TBD|TBD\n\n\n#####Table BA3:  Data Dictionary for State Trait Anxiety Inventory (STAI)\nEprime Variable|Used in Analysis|English Name|Description\n-|-|-|- \nTBD|Y|TBD|TBD\n\n\n#####Table BA4:  Data Dictionary for Profile of Mood States (POMS) \nEprime Variable|Used in Analysis|English Name|Description\n-|-|-|- \nTBD|Y|TBD|TBD\n\n\n#####Table BA5:  Data Dictionary for Dispositional Resilience Scale (DRS) \nEprime Variable|Used in Analysis|English Name|Description\n-|-|-|- \nTBD|Y|TBD|TBD\n\n\n#####Table EF1:  Data Dictionary for Eriksen Flanker Task \nEprime Variable|Used in Analysis|English Name|Description\n-|-|-|-\nExperimentName|Y|Experiment Name|......\nSubject|Y|Subject|......\nSession|Y|Session|......\nClock.Information||TBD|TBD\nDataFile.Basename|Y|DataFile|......\nDisplay.RefreshRate||TBD|TBD\nExperimentVersion||TBD|TBD\nGroup||TBD|TBD\nRandomSeed||TBD|TBD\nRuntimeCapabilities||TBD|TBD\nRuntimeVersion||TBD|TBD\nRuntimeVersionExpected||TBD|TBD\nSessionDate|Y|TBD|......\nSessionStartDateTimeUtc||TBD|TBD\nSessionTime|Y|TBD|......\nStudioVersion||TBD|TBD\nBlock||TBD|TBD\nefBlockList||TBD|TBD\nefBlockList.Cycle||TBD|TBD\nefBlockList.Sample||TBD|TBD\nProcedure[Block]||TBD|TBD\nRunning[Block]||TBD|TBD\nTrial||TBD|TBD\nCorrectAnswerPractice||TBD|TBD\nefStimDisplayPractice.ACC|Y|TBD|......\nefStimDisplayPractice.CRESP|Y|TBD|......\nefStimDisplayPractice.DurationError||TBD|TBD\nefStimDisplayPractice.OnsetDelay||TBD|TBD\nefStimDisplayPractice.OnsetTime||TBD|TBD\nefStimDisplayPractice.OnsetToOnsetTime||TBD|TBD\nefStimDisplayPractice.RESP||TBD|TBD\nefStimDisplayPractice.RT|Y|ResponseTim|......\nefStimDisplayPractice.RTTime||TBD|TBD\nefTrialListPractice||TBD|TBD\nefTrialListPractice.Cycle||TBD|TBD\nefTrialListPractice.Sample|Y|TBD|......\nProcedure[Trial]||TBD|TBD\nRunning[Trial]||TBD|TBD\nStimulusPractice|Y|TBD|TBD\n\n\n#####Table EF2:  Data Dictionary for Iowa Gambling Test\nEprime Variable|Used in Analysis|English Name|Description\n-|-|-|- \nTBD|Y|TBD|TBD\n\n#####Table EF3:  Data Dictionary for N-Back Test\nEprime Variable|Used in Analysis|English Name|Description\n-|-|-|- \nTBD|Y|TBD|TBD\n\n",
    "created" : 1447257263596.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "463987733",
    "id" : "BB52B739",
    "lastKnownWriteTime" : 1447257850,
    "path" : "~/_data/adaptability/Adaptability - Markdown File - Version 30July2015.Rmd",
    "project_path" : "Adaptability - Markdown File - Version 30July2015.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}